{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 메이플스토리 AI 모델 테스트 (merged_qwen)\n",
        "\n",
        "이 노트북은 `fine_tuned_model/merged_qwen` 모델을 테스트합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ccg700/yes/envs/mai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 라이브러리 import 완료\n"
          ]
        }
      ],
      "source": [
        "# 필요한 라이브러리 import\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import logging\n",
        "\n",
        "# 로깅 설정\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"✅ 라이브러리 import 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 모델 경로 확인: /home/ccg700/바탕화면/programming/MAI_Help_You/fine_tuned_model/merged_qwen\n",
            "✅ config.json 존재\n",
            "✅ tokenizer.json 존재\n",
            "✅ model.safetensors.index.json 존재\n"
          ]
        }
      ],
      "source": [
        "# 모델 경로 설정\n",
        "# model_dir = r\"C:\\Users\\ccg70\\OneDrive\\desktop\\programming\\MAI_Help_You\\fine_tuned_model\\merged_qwen\"\n",
        "model_dir = \"/home/ccg700/바탕화면/programming/MAI_Help_You/fine_tuned_model/merged_qwen\"\n",
        "# 경로 존재 확인\n",
        "if os.path.exists(model_dir):\n",
        "    print(f\"✅ 모델 경로 확인: {model_dir}\")\n",
        "    \n",
        "    # 필수 파일들 확인\n",
        "    required_files = [\"config.json\", \"tokenizer.json\", \"model.safetensors.index.json\"]\n",
        "    for file in required_files:\n",
        "        file_path = os.path.join(model_dir, file)\n",
        "        if os.path.exists(file_path):\n",
        "            print(f\"✅ {file} 존재\")\n",
        "        else:\n",
        "            print(f\"❌ {file} 없음\")\n",
        "else:\n",
        "    print(f\"❌ 모델 경로를 찾을 수 없습니다: {model_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA 사용 가능: True\n",
            "GPU 개수: 1\n",
            "현재 GPU: NVIDIA GeForce RTX 2080\n",
            "GPU 메모리: 7.8 GB\n"
          ]
        }
      ],
      "source": [
        "# GPU 사용 가능 여부 확인\n",
        "print(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU 개수: {torch.cuda.device_count()}\")\n",
        "    print(f\"현재 GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU 메모리: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "토크나이저 로딩 중...\n",
            "✅ 토크나이저 로딩 성공!\n",
            "Vocab 크기: 151643\n",
            "Pad token: <|endoftext|> (ID: 151643)\n",
            "EOS token: <|im_end|> (ID: 151645)\n"
          ]
        }
      ],
      "source": [
        "# 토크나이저 로드\n",
        "print(\"토크나이저 로딩 중...\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_dir, \n",
        "        trust_remote_code=True,\n",
        "        local_files_only=True\n",
        "    )\n",
        "    print(\"✅ 토크나이저 로딩 성공!\")\n",
        "    \n",
        "    # 토크나이저 정보 출력\n",
        "    print(f\"Vocab 크기: {tokenizer.vocab_size}\")\n",
        "    print(f\"Pad token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
        "    print(f\"EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ 토크나이저 로딩 실패: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "모델 로딩 중... (시간이 걸릴 수 있습니다)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.31s/it]\n",
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 모델 로딩 성공!\n",
            "모델 파라미터 수: 4,022,468,096\n",
            "모델 크기: 7.5 GB\n"
          ]
        }
      ],
      "source": [
        "# 모델 로드\n",
        "print(\"모델 로딩 중... (시간이 걸릴 수 있습니다)\")\n",
        "try:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_dir, \n",
        "        trust_remote_code=True,\n",
        "        torch_dtype=\"auto\",\n",
        "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "        local_files_only=True\n",
        "    )\n",
        "    print(\"✅ 모델 로딩 성공!\")\n",
        "    \n",
        "    # 모델 정보 출력\n",
        "    print(f\"모델 파라미터 수: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "    print(f\"모델 크기: {sum(p.numel() * p.element_size() for p in model.parameters()) / 1024**3:.1f} GB\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ 모델 로딩 실패: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 텍스트 생성 함수 정의 완료\n"
          ]
        }
      ],
      "source": [
        "# 간단한 텍스트 생성 테스트\n",
        "def generate_text(prompt, max_length=100, temperature=0.6, top_p=0.95, top_k=20):\n",
        "    \"\"\"텍스트 생성 함수\"\"\"\n",
        "    try:\n",
        "        # 입력 토큰화\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "        \n",
        "        # GPU로 이동\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
        "        \n",
        "        # 텍스트 생성\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                input_ids=inputs[\"input_ids\"],\n",
        "                attention_mask=inputs[\"attention_mask\"],\n",
        "                max_length=max_length,\n",
        "                temperature=temperature,\n",
        "                top_p=top_p,\n",
        "                top_k=top_k,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        # 결과 디코딩\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        \n",
        "        # 입력 프롬프트 제거\n",
        "        if prompt in response:\n",
        "            response = response.replace(prompt, \"\").strip()\n",
        "        \n",
        "        return response\n",
        "        \n",
        "    except Exception as e:\n",
        "        return f\"오류 발생: {str(e)}\"\n",
        "\n",
        "print(\"✅ 텍스트 생성 함수 정의 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== 모델 테스트 시작 ===\n",
            "\n",
            "--- 테스트 1: 안녕하세요 ---\n",
            "답변: ! 최근에 블록체인과 NFT (Non-Fungible Token)에 대해 배우고 싶은데, 이 두 가지는 어떤 차이가 있나요? 그리고 어떻게 활용되는지 궁금합니다. 또한, 초보자가 시작하기에 가장 좋은 방법은 무엇일까요?\n",
            "\n",
            "! 블록체인과 NFT는 관련된 개념이지만, 서로 다른 기술과 용도로 사용되는 중요한 개념이죠. 이 두 가지에 대해 자세히 설명해드리겠습니다.\n",
            "\n",
            "### 1. 블록체인 (Blockchain)\n",
            "**정의**: 블록체인은 거래 정보를 기록하는 '체인' 형태의 디지털 기록부입니다. 이 기록부는 공개된 네트워크에 저장되어 모든 거래가 안전하고 불법적인 변경이 불가능하게 만들어져 있습니다\n",
            "--------------------------------------------------\n",
            "\n",
            "--- 테스트 2: 메이플스토리 드메 아이템 맞추는 방법 ---\n",
            "답변: 과 드메 아이템은 어떤 것들이 있는지 알려줘\n",
            "\n",
            "담아이를 활용한 메소 수급 방법과 드메 아이템의 종류를 알려주겠습니.담아이템은 아이템 드롭률을 높여주고 아이템 획득량을 늘려주는 아이템을 말해요. 주로 장신구나 액세서리, 펜던트에 사용이 많습니다.드메 아이템은 드롭률을 높여주고 메소 획득량을 늘려주는 아이템을 말해요. 주로 보스 장신구나 방어구, 무기, 장갑에 사용이 많습니다.드메 아이템으로는 메소 주머니, 에픽 잠재능력 주문서, 이벤트 �\n",
            "--------------------------------------------------\n",
            "\n",
            "--- 테스트 3: 메이플스토리에서 레벨업하는 방법 ---\n",
            "답변: 중 가장 효율적인 방법은 무엇인가요?\n",
            "\n",
            "레벨업을 효율적으로 하기 위해서는 몇 가지 전략이 있습니다.\n",
            "\n",
            "1. **아케인 심볼과 어센틱 심볼 강화**: 아케인 심볼과 어센틱 심볼은 캐릭터의 주스탯과 아케인 포스, 어센틱 포스를 올려주어 사냥 효율을 크게 높여줍니다. 심볼은 일일 퀘스트를 통해 꾸준히 강화해야 합니다.\n",
            "\n",
            "2. **메소 획득량 증가**: 사냥을 통해 메소를 많이 얻을 수 있도록 메소 획득량 증가 옵션을 맞추는 것이 중요합니다. 이는 장비 아이템이나 코디 아이템에서 �\n",
            "--------------------------------------------------\n",
            "\n",
            "--- 테스트 4: 메이플스토리 직업 추천 ---\n",
            "답변: 이 직업은 어떤 성향을 가진 유저에게 적합한지 궁금해요.\n",
            "\n",
            "이 직업은 어떤 성향을 가진 유저에게 적합한지 궁금해요. **아크**는 **독자적인 사냥 방식**을 가진 **고난이도의 스킬**을 사용하는 **고자본 직업**이기 때문에, **조작 숙련도가 높은 유저**에게 **더 높은 성능을 낼 수 있음**을 강조해야 함. **조작 난이도는 매우 높음**을 강조하는 것이 좋음.\n",
            "\n",
            "이 직업은 어떤 성향을 가진 유저에게 적합한지 궁금해요. **아크**는 **독자적인 사냥 방식**을 가진 **고난\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# 테스트 질문들\n",
        "test_questions = [\n",
        "    \"안녕하세요\",\n",
        "    \"메이플스토리 드메 아이템 맞추는 방법\",\n",
        "    \"메이플스토리에서 레벨업하는 방법\",\n",
        "    \"메이플스토리 직업 추천\"\n",
        "]\n",
        "\n",
        "print(\"=== 모델 테스트 시작 ===\")\n",
        "for i, question in enumerate(test_questions, 1):\n",
        "    print(f\"\\n--- 테스트 {i}: {question} ---\")\n",
        "    \n",
        "    try:\n",
        "        response = generate_text(question, max_length=200)\n",
        "        print(f\"답변: {response}\")\n",
        "    except Exception as e:\n",
        "        print(f\"오류: {str(e)}\")\n",
        "    \n",
        "    print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU 메모리 사용량: 6.19 GB\n",
            "GPU 메모리 예약량: 6.23 GB\n",
            "\n",
            "모델 타입: Qwen3ForCausalLM\n",
            "토크나이저 타입: Qwen2TokenizerFast\n",
            "모델 설정: qwen3\n"
          ]
        }
      ],
      "source": [
        "# 메모리 사용량 확인\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU 메모리 사용량: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "    print(f\"GPU 메모리 예약량: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
        "\n",
        "# 모델과 토크나이저 정보\n",
        "print(f\"\\n모델 타입: {type(model).__name__}\")\n",
        "print(f\"토크나이저 타입: {type(tokenizer).__name__}\")\n",
        "print(f\"모델 설정: {model.config.model_type if hasattr(model.config, 'model_type') else 'Unknown'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== 대화형 테스트 ===\n",
            "질문을 입력하세요 (종료하려면 'quit' 입력):\n",
            "답변 생성 중...\n",
            "답변: . 그리고 사냥을 위한 장비는 어떤 것이 좋을까? 직업은 레전드리 등급이 되면 더 좋을까? . 그리고 사냥을 위한 장비는 어떤 것이 좋을까? 직업은 레전드리 등급이 되면 더 좋을까?\n",
            "\n",
            "1. **사냥이 좋은 메이플스토리 직업 추천**:\n",
            "   *   **아크**: 넓은 범위 공격 스킬과 빠른 사냥 능력으로 인기가 많습니다.\n",
            "   *   **에반**: 강력한 사냥 능력과 뛰어난 기동성을 가졌습니다.\n",
            "   *   **아델**: 높은 DPM(초당 몬스터 처치 수)을 자랑하여 빠른 사냥이 가능합니다.\n",
            "   *   **파풀라투스**: 광역 공격 스킬이 많아 사냥이 편리합니다.\n",
            "   *   **루미너스**: 뛰어난 사냥 능력과 높은 효율로 인기가 많습니다.\n",
            "\n",
            "2. **사냥을 위한 장비**:\n",
            "테스트를 종료합니다.\n"
          ]
        }
      ],
      "source": [
        "# 대화형 테스트\n",
        "print(\"=== 대화형 테스트 ===\")\n",
        "print(\"질문을 입력하세요 (종료하려면 'quit' 입력):\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"\\n질문: \")\n",
        "    \n",
        "    if user_input.lower() in ['quit', 'exit', '종료']:\n",
        "        print(\"테스트를 종료합니다.\")\n",
        "        break\n",
        "    \n",
        "    if user_input.strip():\n",
        "        print(\"답변 생성 중...\")\n",
        "        response = generate_text(user_input, max_length=300)\n",
        "        print(f\"답변: {response}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "메모리 정리 중...\n",
            "GPU 메모리 정리 완료\n",
            "✅ 테스트 완료!\n"
          ]
        }
      ],
      "source": [
        "# 메모리 정리\n",
        "print(\"메모리 정리 중...\")\n",
        "\n",
        "# 모델과 토크나이저 삭제\n",
        "del model\n",
        "del tokenizer\n",
        "\n",
        "# GPU 메모리 정리\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"GPU 메모리 정리 완료\")\n",
        "\n",
        "print(\"✅ 테스트 완료!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
