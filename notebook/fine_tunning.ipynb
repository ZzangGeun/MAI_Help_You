{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "916a8c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\local_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built with CUDA: True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# CUDA 빌드 여부 확인 (TensorFlow 2.x)\n",
    "print(f\"Built with CUDA: {tf.test.is_built_with_cuda()}\")\n",
    "\n",
    "# CUDA 빌드 여부 확인 (PyTorch)\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d2fbcf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.4: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3050. Num GPUs = 1. Max memory: 6.0 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu118. CUDA: 8.6. CUDA Toolkit: 11.8. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.5.4 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "import unsloth\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "# 최대 시퀀스 길이를 설정합니다. 내부적으로 RoPE 스케일링을 자동으로 지원합니다!\n",
    "max_seq_length = 2048  # 공식 튜토리얼은 2048\n",
    "# 자동 감지를 위해 None을 사용합니다. Tesla T4, V100은 Float16, Ampere+는 Bfloat16을 사용하세요.\n",
    "dtype = None\n",
    "# 메모리 사용량을 줄이기 위해 4bit 양자화를 사용합니다. False일 수도 있습니다.\n",
    "load_in_4bit = True\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\", # Unsloth가 제공하는 최적화된 모델 사용 가능\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  # 0보다 큰 어떤 숫자도 선택 가능! 8, 16, 32, 64, 128이 권장됩니다.\n",
    "    lora_alpha=32,  # LoRA 알파 값을 설정합니다. # 튜토리얼은 16\n",
    "    lora_dropout=0.05,  # 드롭아웃을 지원합니다. # # Supports any, but = 0 is optimized\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],  # 타겟 모듈을 지정합니다.\n",
    "    bias=\"none\",  # 바이어스를 지원합니다.\n",
    "    # True 또는 \"unsloth\"를 사용하여 매우 긴 컨텍스트에 대해 VRAM을 30% 덜 사용하고, 2배 더 큰 배치 크기를 지원합니다.\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=123,  # 난수 상태를 설정합니다. # 공식 : 3407\n",
    "    use_rslora=False,  # 순위 안정화 LoRA를 지원합니다.\n",
    "    loftq_config=None,  # LoftQ를 지원합니다.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a818a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 입력:\n",
      "리눅스를 잘 다루기 위해서는 어떤 것들을 알아야 할까요 ?\n",
      "\n",
      "모델 응답:\n",
      "리눅스를 잘 다루기 위해서는 어떤 것들을 알아야 할까요? \n",
      "1.  리눅스 시스템의 기본적인 workings을 이해해야 합니다.\n",
      "2.  리눅스에 대한 kiến지의를 쌓아야 합니다.\n",
      "3.  리눅스에 대한 지식과 경험\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 테스트를 위한 프롬프트 준비\n",
    "prompt = \"\"\"리눅스를 잘 다루기 위해서는 어떤 것들을 알아야 할까요 ?\"\"\" # 테스트하고 싶은 질문으로 변경 가능\n",
    "\n",
    "# Unsloth에서 제공하는 프롬프트 형식을 사용할 수도 있습니다.\n",
    "# 예시: Alpaca 형식 (모델이 Alpaca 형식으로 미세 조정되었다고 가정)\n",
    "# alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "# ### Instruction:\n",
    "# {}\n",
    "\n",
    "# ### Response:\n",
    "# {}\"\"\"\n",
    "# inputs = tokenizer(\n",
    "# [\n",
    "#     alpaca_prompt.format(\n",
    "#         \"당신은 누구인가요?\", # instruction\n",
    "#         \"\", # output - 생성 부분이므로 비워둡니다.\n",
    "#     )\n",
    "# ], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "# 간단한 프롬프트를 직접 토큰화\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# 모델을 사용하여 응답 생성\n",
    "# FastLanguageModel은 내부적으로 model.generate를 최적화할 수 있습니다.\n",
    "# generate_params = {\"max_new_tokens\": 50, \"use_cache\": True} # 필요한 경우 생성 파라미터 조정\n",
    "# outputs = model.generate(**inputs, **generate_params)\n",
    "\n",
    "# 더 간단한 방식\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "\n",
    "\n",
    "# 생성된 응답 디코딩\n",
    "response_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"모델 입력:\")\n",
    "print(prompt)\n",
    "print(\"\\n모델 응답:\")\n",
    "print(response_text)\n",
    "\n",
    "# 메모리 정리 (필요한 경우)\n",
    "# import gc\n",
    "# del inputs, outputs\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "# </VSCode.Cell>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e3ab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
